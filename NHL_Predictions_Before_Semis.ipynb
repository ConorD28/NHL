{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ConorD28/NHL/blob/main/NHL_Predictions_Before_Semis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rq6xsPJUvkXO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.decomposition import PCA\n",
        "%matplotlib inline\n",
        "df = pd.read_csv('NHL Upload.csv')\n",
        "df.drop(df.tail(1).index,\n",
        "        inplace = True)\n",
        "df_NP = df.drop(columns=df.columns[-12:], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_NP"
      ],
      "metadata": {
        "id": "hcHETh8zkSZV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_NP.isnull().sum().sum() #Check if there are NA values"
      ],
      "metadata": {
        "id": "C8PEra5HfdD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7966bb6-a90b-42e3-e8bd-23d835f9af79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2LtJLaEVvqfY"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "def correlation(dataset, threshold, target): #Function to get Pearson's correlation between input and target\n",
        "  data = []\n",
        "  for i in range(len(dataset.columns)):\n",
        "      cor2 = dataset.iloc[:,i].corr(target) #scipy.stats.spearmanr(x, y)[0] and scipy.stats.kendalltau(x, y)[0]\n",
        "      column_headers = list(dataset.columns.values)\n",
        "      if(abs(cor2) > threshold):\n",
        "        data.append(dataset.iloc[:,i]) #make list of columns that meet the threshold\n",
        "      i = i + 1\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ngjdaqWIvssV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random.mtrand import random_sample\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV, MultiTaskLassoCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C5q3JY8L1e1M"
      },
      "outputs": [],
      "source": [
        "def Scores(y, y_pred, y_full):\n",
        "  MSE = mean_squared_error(y, y_pred)\n",
        "  MAE = mean_absolute_error(y, y_pred)\n",
        "  Normalized_RMSE = (np.sqrt(MSE)/np.mean(y_full))*100\n",
        "  Normalized_MAE = (MAE/np.mean(y_full))*100\n",
        "  Avg_Normalized_Score = (Normalized_RMSE + Normalized_MAE)/2\n",
        "  avg_error = ((sum(abs(y_pred-y)))/10)*100\n",
        "  print(f'Avg. Normalized Score:{ Avg_Normalized_Score:.1f}%')\n",
        "  print(f'Normalized RMSE:{ Normalized_RMSE:.1f}%')\n",
        "  print(f'Normalized MAE:{ Normalized_MAE:.2f}%')\n",
        "  #print(f'MAE:{ MAE:.3f}')\n",
        "  #print(f'RMSE:{ np.sqrt(MSE):.3f}')\n",
        "  print(f'Avg. Error:{avg_error:.1f}%')\n",
        "  #print(y_pred-y)\n",
        "  return Avg_Normalized_Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zLVl7ooxvt7q"
      },
      "outputs": [],
      "source": [
        "def RLE_Model(X, y, choice, predict_df): #Function to run Ridge, Lasso, or ElasticNet model\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "\n",
        "  if(choice==\"Ridge\"):\n",
        "    alphas = np.geomspace(1e-10, 1e10, num=100)\n",
        "    pipeline = make_pipeline(RidgeCV(alphas=alphas))\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "  if(choice==\"Lasso\"):\n",
        "    alphas = np.geomspace(1e-10, 1e10, num=100)\n",
        "    pipeline = make_pipeline(LassoCV(alphas=alphas))\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "  if(choice==\"Elastic\"):\n",
        "    l1_ratio = [0, 0.3, 0.5, 0.7, 0.9, 1]\n",
        "    alphas = np.geomspace(1e-10, 1e10, num=100)\n",
        "    pipeline = make_pipeline(ElasticNetCV(alphas=alphas, l1_ratio=l1_ratio, max_iter=100000))\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "  #print(f'Chosen alpha  {pipeline.steps[0][1].alpha_:.6f}')\n",
        "  #print(f'Intercept (b) {pipeline.steps[0][1].intercept_:.6f}')\n",
        "  #print(pd.Series(pipeline.steps[0][1].coef_, index=X.columns),'\\n')\n",
        "\n",
        "  #Calculate the predicted values:\n",
        "  y_train_pred = pipeline.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "  print()\n",
        "\n",
        "  y_test_pred = pipeline.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Test Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #print(sum(abs(y_test_pred - y_test))/10)\n",
        "\n",
        "  #Predict:\n",
        "  predictions = pipeline.predict(predict_df)\n",
        "\n",
        "  return y_test_pred, y_train_pred, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FpU87Mgvvx0s"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def GBR_model(X,y, t, l, n, predict_df):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  reg = GradientBoostingRegressor(tol = t, learning_rate = l, n_estimators=n, random_state=0) #default: tol = 0.0001, learning rate - 0.1, 100, friedman_mse\n",
        "  reg.fit(X_train, y_train)\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "\n",
        "  print(\"Train Scores:\")\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  print()\n",
        "\n",
        "  #Predictions:\n",
        "  #print(\"Test predictions:\")\n",
        "  #print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  print(\"Test Scores:\")\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #Predict:\n",
        "  predictions = reg.predict(predict_df)\n",
        "  return y_test_pred, y_train_pred, predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "\n",
        "def BR_model(X,y):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  reg = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        "  #reg = pickle.load(open(\"BR_model Per 100\", \"rb\"))\n",
        "  reg.fit(X_train, y_train)\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #Predict:\n",
        "  #predictions = reg.predict(predict_df)\n",
        "  #print(predictions)\n",
        "\n",
        "  pickle.dump(reg, open(\"BR_model P%\", \"wb\"))\n",
        "  return y_test_pred, y_train_pred"
      ],
      "metadata": {
        "id": "ppib1skHfgGk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XfM5w0Ncvynu"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#SGD Regressor:\n",
        "def SGD_model(X,y, t, ep):\n",
        "\n",
        "  reg = make_pipeline(SGDRegressor(max_iter=1000, tol=t, epsilon = ep)) #tol = 0.001, epsilon=0.1\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  reg.fit(X_train, y_train)\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  #print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #predictions = reg.predict(predict_df)\n",
        "  #print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras Sequential Neural Net\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=2)\n",
        "\n",
        "def Keras_model(X,y,e, u, u2, u3, u4, u5):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  model = Sequential()\n",
        "  model.add(Dense(u, input_dim=X_train.shape[1], activation='relu')) # Hidden 1, 60\n",
        "  model.add(Dense(units=u2,activation='relu')) # Hidden 2, 30\n",
        "  model.add(Dense(units=u3,activation='relu'))\n",
        "  model.add(Dense(units=u4,activation='relu'))\n",
        "  model.add(Dense(units=u5,activation='relu'))\n",
        "  model.add(Dense(units=15,activation='relu')) #15\n",
        "  model.add(Dense(units=1)) #,activation='relu'\n",
        "  model.compile(loss='mean_squared_error', optimizer='adam') #adam, nadam; adamax\n",
        "  model.fit(X_train, y_train, verbose=0, epochs=e, callbacks=[early_stop]); #callbacks=[early_stop]\n",
        "\n",
        "  y_train_pred = model.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = model.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  #print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #model.save('/content/drive/MyDrive/Models/Keras_Model', save_format=\"h5\")"
      ],
      "metadata": {
        "id": "Zzt-o4jPgtdH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "def DTR_model(X,y,leafs):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  # We introduce regularization by increasing the value of min_samples_leaf\n",
        "  tree_reg_regularized = DecisionTreeRegressor(random_state=42, min_samples_leaf=leafs)\n",
        "  tree_reg_regularized.fit(X_train, y_train)\n",
        "  y_train_pred = tree_reg_regularized.predict(X_train) #_regularized\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = tree_reg_regularized.predict(X_test) #_regularized\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()"
      ],
      "metadata": {
        "id": "sOg9t7czmLFe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "def SVM_model(X,y,ep, predict_df):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  svm_reg = LinearSVR(epsilon=ep, random_state=42) #default: epsilon = 0 tol=0.0001, C=1.0\n",
        "  svm_reg.fit(X_train, y_train)\n",
        "\n",
        "  #Train Predictions:\n",
        "  y_train_pred = svm_reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  print(\"Train Scores:\")\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  print()\n",
        "\n",
        "  #Test Predictions:\n",
        "  y_test_pred = svm_reg.predict(X_test)\n",
        "  #print(\"Test predictions:\")\n",
        "  #print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  print(\"Test Scores:\")\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  final_preds = svm_reg.predict(predict_df)\n",
        "\n",
        "  return y_test_pred, y_train_pred, final_preds"
      ],
      "metadata": {
        "id": "luksQpOa9mx6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "def SVM_models(X,y, choice, ep, C_value, predict_df):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "\n",
        "  if(choice==\"rbf\"):\n",
        "    model = SVR(kernel=\"rbf\", C=C_value, gamma=0.1, epsilon=ep) #0.1 default ep; 100 default C, 0.1 default gamma\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "  if(choice==\"poly\"):\n",
        "    model = SVR(kernel=\"poly\", C=C_value, gamma=\"auto\", degree=3, epsilon=ep, coef0=1) #0.1 default ep; 100 default C\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "  if(choice == \"linear\"):\n",
        "    model = SVR(kernel=\"linear\", C=C_value, gamma=\"auto\", degree=3, epsilon=ep, coef0=1) #0.1 default ep; 100 default C\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "  #Train Predictions:\n",
        "  y_train_pred = model.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Test Predictions:\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.3f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  final_preds = model.predict(predict_df)\n",
        "\n",
        "  return y_test_pred, y_train_pred, final_preds"
      ],
      "metadata": {
        "id": "1ms2SSQ_3ncH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "from sklearn.manifold import LocallyLinearEmbedding"
      ],
      "metadata": {
        "id": "UhxkZpeNTZA7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:, 203:]"
      ],
      "metadata": {
        "id": "GQ1MImLOcqjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "RJSe6xMvwg_n",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_correlated_df2.columns"
      ],
      "metadata": {
        "id": "01TJgyHv_ndO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Importance:\n",
        "scaler = StandardScaler() #StandardScaler(), MinMaxScaler()\n",
        "data_scaled = pd.DataFrame(scaler.fit_transform(df_NP[10:30]), columns = df_NP.columns)\n",
        "data_correlated = correlation(data_scaled, .395, df['P%_Playoffs']) #.115 lowest for All csv\n",
        "data_correlated_df = pd.DataFrame(data_correlated)\n",
        "data_correlated_df2 = data_correlated_df.transpose() #Correlated inputs\n",
        "data_correlated_df2.insert(26, \"Win%/SOS_Traditional\", data_scaled[\"Win%/SOS_Traditional\"])\n",
        "data_correlated_df2.insert(27, \"P%/SOS_Traditional\", data_scaled[\"P%/SOS_Traditional\"])\n",
        "data_correlated_df2.insert(28, \"Net PP%/SOS_Traditional\", data_scaled[\"Net PP%/SOS_Traditional\"])\n",
        "X = df_NP.loc[:, data_correlated_df2.columns] #get non scaled data with important features\n",
        "\n",
        "#Train test split and scale:\n",
        "X_train, X_test = X[10:30], X[0:10]\n",
        "X_train_processed = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
        "X_test_processed = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
        "correlated_scaled_data = pd.merge(X_test_processed, X_train_processed, how = 'outer')\n",
        "#correlated_scaled_data_MMS = pd.merge(X_test_processed, X_train_processed, how = 'outer')\n",
        "\n",
        "#PCA:\n",
        "pca=PCA(n_components = 4)\n",
        "X_train_processed_PCA = pca.fit_transform(X_train_processed)\n",
        "X_train_PCA_df = pd.DataFrame(X_train_processed_PCA)\n",
        "X_test_processed_PCA = pca.transform(X_test_processed)\n",
        "X_test_PCA_df = pd.DataFrame(X_test_processed_PCA)\n",
        "data_PCA = pd.merge(X_test_PCA_df, X_train_PCA_df, how = 'outer')\n",
        "#data_PCA_MMS = pd.merge(X_test_PCA_df, X_train_PCA_df, how = 'outer')\n",
        "print(\"Principal axes:\\n\", pca.components_.tolist())\n",
        "print(\"Explained variance:\\n\", pca.explained_variance_.tolist())\n",
        "print(\"Mean:\", pca.mean_)\n",
        "\n",
        "#LLE:\n",
        "#X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=15, random_state=42) #n_components=2 is default, neighbors 5 is default\n",
        "X_unrolled_train = lle.fit_transform(X_train_processed)\n",
        "X_train_LLE_df = pd.DataFrame(X_unrolled_train)\n",
        "X_unrolled_test = lle.transform(X_test_processed)\n",
        "X_test_LLE_df = pd.DataFrame(X_unrolled_test)\n",
        "data_LLE = pd.merge(X_test_LLE_df, X_train_LLE_df, how = 'outer')\n",
        "#data_LLE_MMS = pd.merge(X_test_LLE_df, X_train_LLE_df, how = 'outer')\n",
        "\n",
        "X = correlated_scaled_data #correlated_scaled_data, data_PCA, or data_LLE\n",
        "y = df['P%_Playoffs'] #"
      ],
      "metadata": {
        "id": "Z0RXs3FQMoHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b41b2de-43d1-4772-c1ed-69d80f8a7fdc",
        "collapsed": true
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Principal axes:\n",
            " [[-0.09013802341165948, 0.1975304691446745, 0.21115387948888392, 0.13408613743206546, 0.11835509119737365, 0.10621484777292775, 0.17087369816064038, 0.10621484777292775, 0.21840077457683682, 0.10621484777292775, -0.11619087477048777, 0.10621484777292775, 0.2117430079280087, 0.22563448542268635, 0.17157520207020574, 0.14252055634144403, 0.12509097107079292, 0.17087369816064038, -0.2435986611851978, 0.249280260145025, 0.24276994160114748, 0.2454896020182969, 0.24432637540437177, 0.24434554828422847, 0.2235145678593981, 0.23770472644853483, 0.18401572305454994, 0.20349960171440362, 0.09273111495505357], [-0.08615481459809368, -0.19328506628696726, 0.17428601846451514, -0.107297862107156, 0.23378632942330002, 0.33257901188482275, 0.18156660568413574, 0.3325790118848227, -0.20542671614058325, 0.3325790118848227, 0.21263106309486993, 0.3325790118848227, 0.17490991443945836, 0.08274000651632911, 0.17787580751893953, 0.024140185251012133, 0.22025364556980337, 0.18156660568413574, 0.12110345952629858, -0.08096545963024006, -0.11983659264067513, -0.09970900638810543, -0.0977131340328302, -0.09773541686389653, -0.1790128268211825, -0.1261095617924675, -0.11574801505613062, -0.0940458077005581, -0.09952042925366895], [-0.16402113219267403, -0.010320130275021626, -0.07246088684097181, -0.2655287959038503, -0.3539319808845058, 0.2512153136463935, -0.29993382186015444, 0.2512153136463935, 0.06262089897030255, 0.2512153136463935, -0.0970355578657723, 0.2512153136463935, -0.0725718113087034, -0.03816174096862982, 0.019063087759316262, -0.14057161250092043, -0.3623774296590095, -0.29993382186015444, -0.0927945829020897, 0.05216633321923649, 0.10369088608162964, 0.08929011210398258, 0.09322714105214894, 0.09306837522920375, 0.028223203340128268, 0.10958752521237798, 0.03072542457744051, 0.0050529805073452425, -0.32699044931953974], [0.24344265987136499, 0.009927671138034427, -0.2295579002235731, -0.37132456368181505, 0.1426043421602817, 0.04417995383074976, 0.15540260626678754, 0.04417995383074968, -0.013870421066328343, 0.04417995383074976, -0.2965611633522893, 0.04417995383074976, -0.21042341670929618, -0.10835437003932401, -0.355766373705956, 0.549158392885188, 0.1305982711442121, 0.15540260626678748, -0.0228489185213217, 0.008363268246634286, 0.04354640639353914, 0.0674354245639801, 0.020422253874210518, 0.020416413876057464, -0.028978480188521834, 0.10649723549861777, -0.043017267621333594, 0.00478867503315364, -0.2496601896522732]]\n",
            "Explained variance:\n",
            " [15.488269540669382, 5.061043947057905, 3.9795590681445008, 1.4708666863510167]\n",
            "Mean: [-2.72004641e-16  1.44328993e-16 -4.96824804e-16 -1.31006317e-15\n",
            "  6.32827124e-16 -2.44249065e-16 -1.22124533e-16 -2.44249065e-16\n",
            " -5.44009282e-16 -2.44249065e-16  7.77156117e-17 -2.44249065e-16\n",
            " -1.94289029e-16 -1.74166237e-16  5.82867088e-17 -3.59434704e-16\n",
            " -9.99200722e-17 -1.22124533e-16 -4.21884749e-16 -1.43218770e-15\n",
            "  1.19904087e-15  1.64313008e-15  6.32827124e-16  6.10622664e-16\n",
            " -4.10782519e-16 -9.65894031e-16  1.77635684e-15 -5.55111512e-16\n",
            "  5.55111512e-16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_correlated_df2.columns)"
      ],
      "metadata": {
        "id": "zCYcvQ1wa8Bn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a061809e-64a9-491d-e273-2f2aff7d1f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**P%**"
      ],
      "metadata": {
        "id": "5shDpDOhXFI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model: (P%_Playoffs) .395\n",
        "RLE_Model(X, y, \"Ridge\", X) #7.6%, 1.7%, PCA, 7.5%, 1.9%, PCA, MMS\n",
        "RLE_Model(X, y, \"Lasso\", X) #8.8%, 1%, corr, MMS\n",
        "SGD_model(X,y, 1e-3, 0.1) #\n",
        "GBR_model(X,y, .0001, 0.01, 100, X) #7.1%, 2.2%, PCA, 7.8%, 2.5%, LLE, 7.4%, 1.7%, LLE, MMS, 7.6%, 2.6%, PCA, MMS\n",
        "print(\"5:\")\n",
        "DTR_model(X,y, 100) #"
      ],
      "metadata": {
        "id": "sZiNfyW6tIpt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_models(X, y, \"poly\", .1, 100, X) #8.3%, 1.6%, PCA, 8.5%, .4%, LLE, MMS, 9.2%, .4%, PCA, MMS, 9.5%, .5%, corr, MMS\n",
        "SVM_models(X, y, \"linear\", .1, 100, X) #9.9%, 1%, corr, 8.7%, 1.1%, LLE, 8.4%, 1.5%, LLE, MMS, 9.5%, .5%, corr, MMS\n",
        "SVM_model(X,y, 0.06) #7.8%, 1.7%, corr, 8%, 1.5%, LLE, 7.7%, 1.8%, LLE, MMS, 9.9%, .4%, PCA, MMS, 6.8%, .4%, corr, MMS"
      ],
      "metadata": {
        "id": "9eGYjBc_-JxT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RLE_Model(X, y, \"Elastic\", X) #8.1%, .9%, PCA, 7.6%, 1.9%, LLE, 7.7%, 1.5%, PCA, MMS, 8.8%, 1.1%, corr, MMS\n",
        "BR_model(X,y) #7.8%, 2.5%, corr, 7.7%, 3.1%, LLE, 8.5%, 3.7%, corr, MMS\n",
        "Keras_model(X, y, 200, 120, 60, 30, 20, 15) #"
      ],
      "metadata": {
        "id": "9sZlCeWYWM01",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#P% Blender: 6.4% avg normalized score, .9% diff, and 3.3% avg error\n",
        "predict_P = pd.read_csv('P%_Edmonton.csv') #\n",
        "\n",
        "X_MMS = correlated_scaled_data_MMS\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit_transform(X_train)\n",
        "scaled_data_MMS = scaler.transform(predict_P) #\n",
        "\n",
        "preds = SVM_model(X_MMS,y, 0.06, scaled_data_MMS) #6.8%, .4%, corr, MMS\n",
        "\n",
        "X_PCA = data_PCA\n",
        "scaler = StandardScaler()\n",
        "X_train_processed = scaler.fit_transform(X_train)\n",
        "pca.fit_transform(X_train_processed)\n",
        "scaled_data = scaler.transform(predict_P)\n",
        "scaled_data_PCA = pca.transform(scaled_data)\n",
        "\n",
        "preds2 = GBR_model(X_PCA,y, .0001, 0.01, 100, scaled_data_PCA) #7.1%, 2.2%, PCA, (8.2)\n",
        "\n",
        "train_preds = (preds[1] + preds2[1])/2\n",
        "\n",
        "test_preds = (preds[0] + preds2[0])/2 #6.4%, .9% (6.9%), 3.3% for 1 and 3\n",
        "\n",
        "print(\"Blender Train Scores then Test Scores:\")\n",
        "Scores(y[10:30], train_preds, y)\n",
        "print()\n",
        "Scores(y[0:10], test_preds, y)\n",
        "print()\n",
        "\n",
        "P_predictions = pd.DataFrame()\n",
        "P_predictions[\"Predictions\"] = (preds[2] + preds2[2])/2\n",
        "P_predictions.to_excel(\"P%_Predictions.xlsx\")"
      ],
      "metadata": {
        "id": "JA896fu7cbDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3612974e-2fbf-4150-8626-1d43addd25b0",
        "collapsed": true
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Scores:\n",
            "Avg. Normalized Score:7.2%\n",
            "Normalized RMSE:7.6%\n",
            "Normalized MAE:6.85%\n",
            "Avg. Error:8.9%\n",
            "\n",
            "Test Scores:\n",
            "Avg. Normalized Score:6.8%\n",
            "Normalized RMSE:7.4%\n",
            "Normalized MAE:6.15%\n",
            "Avg. Error:4.0%\n",
            "Difference of avg scores:-0.413%\n",
            "\n",
            "Train Scores:\n",
            "Avg. Normalized Score:4.9%\n",
            "Normalized RMSE:5.3%\n",
            "Normalized MAE:4.51%\n",
            "Avg. Error:5.9%\n",
            "\n",
            "Test Scores:\n",
            "Avg. Normalized Score:7.1%\n",
            "Normalized RMSE:8.9%\n",
            "Normalized MAE:5.41%\n",
            "Avg. Error:3.5%\n",
            "Difference of avg scores:2.243%\n",
            "\n",
            "Blender Train Scores then Test Scores:\n",
            "Avg. Normalized Score:5.5%\n",
            "Normalized RMSE:5.9%\n",
            "Normalized MAE:5.09%\n",
            "Avg. Error:6.6%\n",
            "\n",
            "Avg. Normalized Score:6.4%\n",
            "Normalized RMSE:7.7%\n",
            "Normalized MAE:5.03%\n",
            "Avg. Error:3.3%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LinearSVR was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Vm8WmlMWl5qPkDh4nqbzn64vOM6ndqxq",
      "authorship_tag": "ABX9TyPBS2SBQ8ueelrIHWQmxddo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}