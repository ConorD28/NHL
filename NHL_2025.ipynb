{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ConorD28/NHL/blob/main/NHL_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq6xsPJUvkXO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "inputs = pd.read_csv('NHL inputs.csv')\n",
        "playoff_stats = pd.read_csv('NHL_playoffs.csv')\n",
        "inputs['Rank'] = playoff_stats['Rank_Playoffs']\n",
        "\n",
        "print(inputs.isnull().sum().sum()) #Check if there are NA values\n",
        "print(playoff_stats.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9zr28DwjQ6D"
      },
      "source": [
        "\n",
        "\n",
        "# **Correlation/Scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2LtJLaEVvqfY"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "def correlation(dataset, threshold, target): #Function to get Pearson's correlation between input and target\n",
        "  data = []\n",
        "  cols = []\n",
        "  correlations = []\n",
        "  #corS = 0\n",
        "  if isinstance(target, np.ndarray):\n",
        "    target = pd.Series(target)\n",
        "  for col in dataset.columns:\n",
        "      #print(dataset.loc[:,col])\n",
        "      #print(col)\n",
        "      corS = dataset.loc[:,col].corr(target, method='spearman') # 'kendall'\n",
        "      corP = dataset.loc[:,col].corr(target)\n",
        "      if (abs(corP) > threshold) or (abs(corS) > threshold):\n",
        "        cor2 = max(abs(corP), abs(corS))\n",
        "        data.append(dataset.loc[:,col]) #make list of columns that meet the threshold\n",
        "        cols.append(col)\n",
        "        correlations.append(cor2) #make list of correlations that meet the threshold\n",
        "  if len(data) == 0:\n",
        "     return pd.DataFrame()\n",
        "\n",
        "  df = pd.DataFrame(data)\n",
        "  df_len = len(df.columns)\n",
        "  df.insert(df_len, 'corrs', correlations)\n",
        "  df = df.sort_values(by=df.columns[-1], ascending=False, key = abs)\n",
        "  df = df.transpose()\n",
        "  df_corrs = df.iloc[-1:, :]\n",
        "  df = df.drop(df.tail(1).index)\n",
        "  return df, df_corrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngjdaqWIvssV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random.mtrand import random_sample\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV, MultiTaskLassoCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5q3JY8L1e1M"
      },
      "outputs": [],
      "source": [
        "def Scores(y, y_pred):\n",
        "  MSE = mean_squared_error(y, y_pred)\n",
        "  MAE = mean_absolute_error(y, y_pred)\n",
        "\n",
        "  range_y = y.max() - y.min()\n",
        "  Normalized_RMSE = (np.sqrt(MSE)/abs(range_y))\n",
        "  Normalized_MAE = (MAE/abs(range_y))\n",
        "  #print(f'Normalized RMSE:{ Normalized_RMSE:.2f}')\n",
        "  #print(f'Normalized MAE:{ Normalized_MAE:.2f}')\n",
        "  #print(f'MAE:{ MAE:.3f}')\n",
        "  #print(f'RMSE:{ np.sqrt(MSE):.3f}')\n",
        "  return Normalized_RMSE, Normalized_MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifryB2OHrVml"
      },
      "outputs": [],
      "source": [
        "def Predict_Scores(model, X_tr, X_te, y_tr, y_te, t_sc):#, predict_df):\n",
        "  y_train_pred = model.predict(X_tr)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  if len(y_te) != 0:\n",
        "    y_test_pred = model.predict(X_te)\n",
        "  else:\n",
        "    y_test_pred = pd.DataFrame()\n",
        "  #print('y test values:')\n",
        "  #print(y_test_pred)\n",
        "\n",
        "  #print('after inverse transform, training off by:')\n",
        "  y_train_pred_transformed = t_sc.inverse_transform(y_train_pred.reshape(-1, 1)) # Reshape y_train_pred\n",
        "  y_train_pred_transformed = pd.Series(y_train_pred_transformed.flatten())\n",
        "  y_tr_transformed = t_sc.inverse_transform(y_tr.values.reshape(-1, 1))\n",
        "  y_tr_transformed = pd.Series(y_tr_transformed.flatten())\n",
        "  inv_error_tr_transformed = np.abs(y_tr_transformed - y_train_pred_transformed)\n",
        "\n",
        "  #print('Training Scores:')\n",
        "  NRMSE_tr, NMAE_tr = Scores(y_tr_transformed, y_train_pred_transformed)\n",
        "\n",
        "  #print('y training values:')\n",
        "  #print(y_train_pred_transformed)\n",
        "\n",
        "  inv_error_test_transformed = 0\n",
        "  y_te_transformed = 0\n",
        "  y_test_pred_transformed = 0\n",
        "  #Test Predictions:\n",
        "  if len(y_te) != 0:\n",
        "    y_te_transformed = t_sc.inverse_transform(y_te.reshape(-1, 1))\n",
        "    y_te_transformed = y_te_transformed.flatten()\n",
        "    y_test_pred_transformed = t_sc.inverse_transform(y_test_pred.reshape(-1, 1))\n",
        "    y_test_pred_transformed = y_test_pred_transformed.flatten()\n",
        "    inv_error_test_transformed = np.abs(y_te_transformed - y_test_pred_transformed)\n",
        "    #print(y_test_pred_transformed)\n",
        "\n",
        "  #Predict:\n",
        "  #predictions = model.predict(predict_df)\n",
        "\n",
        "  return NRMSE_tr, NMAE_tr, inv_error_tr_transformed, inv_error_test_transformed, y_test_pred_transformed, y_train_pred_transformed#, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhvX9hZQi5U7"
      },
      "source": [
        "# **ML Tuning Algorithms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d3UKVx4RG1TU"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "import optuna\n",
        "!pip install joblib\n",
        "import joblib\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Cvp4C7sqoRZx"
      },
      "outputs": [],
      "source": [
        "def Ridge_tune(X_train, y_train, cv_choice, num_trials, timeout_choice):\n",
        "    def objective(trial, cv_runs, X_train, y_train):\n",
        "      alpha = trial.suggest_float(\"alpha\", 5, 20, log=True)#1e-4, 10.0; Alpha is the regularization strength\n",
        "      solver = trial.suggest_categorical(\"solver\", [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"])\n",
        "\n",
        "      model = Ridge(alpha=alpha, solver=solver, random_state=28)\n",
        "      score = cross_val_score(model, X_train, y_train, cv=cv_runs, scoring=\"neg_root_mean_squared_error\").mean()\n",
        "      return -score  # Minimize the MSE\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(lambda trial: objective(trial, cv_choice, X_train, y_train), n_trials=num_trials, timeout=timeout_choice)\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(f\"  Params: {trial.params}\")\n",
        "\n",
        "    best_model = Ridge(**trial.params, random_state=28)\n",
        "    best_model.fit(X_train, y_train)\n",
        "    #joblib.dump(best_model, 'PPG_Ridge.pkl')\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O8UenllL9Tt"
      },
      "outputs": [],
      "source": [
        "def Lasso_tune(X_train, y_train, cv_choice, num_trials, timeout_choice):\n",
        "  def Lasso_objective(trial, cv_runs, X_train, y_train):\n",
        "    alpha = trial.suggest_float(\"alpha\", 1e-2, 10, log=True)##1e-4, 10.0;\n",
        "    max_iter = trial.suggest_int(\"max_iter\", 1000, 10000, step=100)  # Max iterations\n",
        "    tol = trial.suggest_float(\"tol\", 1e-5, 1e-2, log=True)  # Tolerance for stopping criteria\n",
        "\n",
        "    model = Lasso(alpha=alpha, max_iter=max_iter, tol=tol, random_state=28)\n",
        "    score = cross_val_score(model, X_train, y_train, cv=cv_runs, scoring=\"neg_root_mean_squared_error\").mean()\n",
        "    return -score  # Minimize the MSE\n",
        "\n",
        "  study = optuna.create_study(direction=\"minimize\")\n",
        "  study.optimize(lambda trial: Lasso_objective(trial, cv_choice, X_train, y_train), n_trials=num_trials, timeout=timeout_choice)\n",
        "  print(\"Best trial:\")\n",
        "  trial = study.best_trial\n",
        "  print(f\"  Params: {trial.params}\")\n",
        "\n",
        "  best_model = Lasso(**trial.params, random_state=28)\n",
        "  best_model.fit(X_train, y_train)\n",
        "  joblib.dump(best_model, 'GF_GP_Lasso.pkl')\n",
        "  return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arifqUP7P65I"
      },
      "outputs": [],
      "source": [
        "def Elastic_tune(X_train, y_train, cv_choice, num_trials, timeout_choice):\n",
        "  def Elastic_objective(trial, cv_runs, X_train, y_train):\n",
        "    alpha = trial.suggest_float(\"alpha\", 1e-1, 10, log=True)#1e-4, 10.0; Regularization strength\n",
        "    l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0)  # Mixing ratio between Lasso and Ridge\n",
        "    max_iter = trial.suggest_int(\"max_iter\", 1000, 10000, step=100)  # Max iterations\n",
        "    tol = trial.suggest_float(\"tol\", 1e-5, 1e-2, log=True)  # Tolerance for stopping criteria\n",
        "\n",
        "    model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=max_iter, tol=tol, random_state=28)\n",
        "    score = cross_val_score(model, X_train, y_train, cv=cv_runs, scoring=\"neg_root_mean_squared_error\").mean()\n",
        "    return -score  # Minimize the MSE\n",
        "\n",
        "  study = optuna.create_study(direction=\"minimize\")\n",
        "  study.optimize(lambda trial: Elastic_objective(trial, cv_choice, X_train, y_train), n_trials=num_trials, timeout=timeout_choice)\n",
        "\n",
        "  print(\"Best trial:\")\n",
        "  trial = study.best_trial\n",
        "  print(f\"  Params: {trial.params}\")\n",
        "\n",
        "  best_model = ElasticNet(**trial.params, random_state=28)\n",
        "  best_model.fit(X_train, y_train)\n",
        "  joblib.dump(best_model, 'GF_GP_Elastic_.pkl')\n",
        "  return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8uNS5w0gAHH"
      },
      "outputs": [],
      "source": [
        "def RF_tune(X_train, y_train, cv_choice, num_trials, timeout_choice):\n",
        "  def RF_objective(trial, cv_runs, X_train, y_train):\n",
        "    n_estimators = trial.suggest_int('n_estimators', 50, 110)#300)\n",
        "    max_depth = trial.suggest_int('max_depth', 5, 45) #50\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 10, 20)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
        "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
        "\n",
        "    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features, random_state=28, n_jobs=-1)\n",
        "\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=cv_runs, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
        "    mean_score = np.mean(scores)\n",
        "    return -mean_score\n",
        "\n",
        "  study = optuna.create_study(direction='minimize')\n",
        "  study.optimize(lambda trial: RF_objective(trial, cv_choice, X_train, y_train), n_trials=num_trials, timeout=timeout_choice)\n",
        "  print(\"Best parameters:\", study.best_params)\n",
        "\n",
        "  best_model = RandomForestRegressor(\n",
        "      n_estimators=study.best_params['n_estimators'], max_depth=study.best_params['max_depth'],\n",
        "      min_samples_split=study.best_params['min_samples_split'], min_samples_leaf=study.best_params['min_samples_leaf'],\n",
        "      max_features=study.best_params['max_features'], random_state=28, n_jobs=-1)\n",
        "  best_model.fit(X_train, y_train)\n",
        "  joblib.dump(best_model, 'GF_GP_RF.pkl')\n",
        "  return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmtzlF9bB8wA"
      },
      "outputs": [],
      "source": [
        "def XG_tune(X_train, y_train, cv_choice, num_trials, timeout_choice):\n",
        "  def XG_objective(trial, cv_runs, X_train, y_train):\n",
        "    params = {\n",
        "        'verbosity': 0,\n",
        "        'objective': 'reg:squarederror',\n",
        "        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),\n",
        "        'lambda': trial.suggest_float('lambda', .5, 10.0, log=True), #1e-3,10\n",
        "        'alpha': trial.suggest_float('alpha', .5, 10.0, log=True), #1e-3,10\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 5), #10\n",
        "        'eta': trial.suggest_float('eta', 1e-3, 0.3, log=True),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 30, 100) #100,1000\n",
        "    }\n",
        "\n",
        "    model = xgb.XGBRegressor(**params, tree_method = 'hist')\n",
        "    scores = cross_val_score(model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=cv_runs\n",
        "  )\n",
        "    return -np.mean(scores)\n",
        "\n",
        "  study = optuna.create_study(direction='minimize')\n",
        "  study.optimize(lambda trial: XG_objective(trial, cv_choice, X_train, y_train), n_trials=num_trials, timeout=timeout_choice)\n",
        "\n",
        "  best_params = study.best_params\n",
        "  best_params['objective'] = 'reg:squarederror'\n",
        "\n",
        "  final_model = xgb.XGBRegressor(**best_params, tree_method = 'hist', random_state=28)\n",
        "  final_model.fit(X_train, y_train)\n",
        "\n",
        "  #joblib.dump(best_model, 'PPG_XG.pkl')\n",
        "  return final_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4dHDJtOsKU_"
      },
      "source": [
        "# **ML Algorithms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLVl7ooxvt7q"
      },
      "outputs": [],
      "source": [
        "def RLE_Model(xTrain, xTest, yTrain, yTest, choice, predict_df, tar_sca): #Function to run Ridge, Lasso, or ElasticNet model\n",
        "  if(choice==\"Ridge\"):\n",
        "    pipeline = Ridge_tune(xTrain, yTrain, 10, 200, 15)\n",
        "\n",
        "  if(choice==\"Lasso\"):\n",
        "    pipeline = Lasso_tune(xTrain, yTrain, 10, 200, 15)\n",
        "\n",
        "  if(choice==\"Elastic\"):\n",
        "    pipeline = Elastic_tune(xTrain, yTrain, 5, 200, 15)\n",
        "\n",
        "  modelResults = Predict_Scores(pipeline, xTrain, xTest, yTrain, yTest, tar_sca)\n",
        "  #print(f'Chosen alpha  {pipeline.steps[0][1].alpha_:.6f}')\n",
        "  #print(f'Intercept (b) {pipeline.steps[0][1].intercept_:.6f}')\n",
        "  #print(pd.Series(pipeline.steps[0][1].coef_, index=X.columns),'\\n')\n",
        "  return modelResults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfB2M7eKfne7"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "def RF_model(xTrain, xTest, yTrain, yTest, predict_df, tar_sca):\n",
        "  model = RF_tune(xTrain, yTrain, 5, 150, 15)\n",
        "  modelResults = Predict_Scores(model, xTrain, xTest, yTrain, yTest, tar_sca)\n",
        "  return modelResults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7yasknHCzSD"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "def XG_model(xTrain, xTest, yTrain, yTest, predict_df, tar_sca):\n",
        "  model = XG_tune(xTrain, yTrain, 5, 200, 15)\n",
        "  modelResults = Predict_Scores(model, xTrain, xTest, yTrain, yTest, tar_sca)\n",
        "  return modelResults"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4QbyjhZi-0Z"
      },
      "source": [
        "# **Inputs/LOOCV Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8haqjJB_Vjz"
      },
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "def corr_matrix_reduce(x_train, x_test):\n",
        "  def compute_corr_row(i, data):\n",
        "      return [data.iloc[:, i].corr(data.iloc[:, j]) for j in range(data.shape[1])]\n",
        "\n",
        "  correlation_matrix = Parallel(n_jobs=-1)(\n",
        "      delayed(compute_corr_row)(i, x_train) for i in range(x_train.shape[1])\n",
        "  )\n",
        "\n",
        "  correlation_matrix = pd.DataFrame(correlation_matrix, columns=x_train.columns, index=x_train.columns)\n",
        "\n",
        "  # Step 2: Reduce features based on correlation threshold\n",
        "  def reduce_features(corr_matrix, threshold=0.9):\n",
        "    #Reduce features by removing one feature from any pair with a correlation above the threshold.\n",
        "      to_drop = set()\n",
        "      for i in range(corr_matrix.shape[0]):\n",
        "          for j in range(i + 1, corr_matrix.shape[1]):\n",
        "              if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "                  # Add the second feature to the drop list\n",
        "                  to_drop.add(corr_matrix.columns[j])\n",
        "      return to_drop\n",
        "\n",
        "  threshold = 0.9\n",
        "  features_to_drop = reduce_features(correlation_matrix, threshold)\n",
        "\n",
        "  # Drop the features from the original dataset\n",
        "  x_train_reduced = x_train.drop(columns=features_to_drop)\n",
        "  if x_test.empty != True:\n",
        "    x_test = x_test.drop(columns=features_to_drop)\n",
        "\n",
        "  # Step 3: Print results\n",
        "  print(\"Original features:\", x_train.shape[1])\n",
        "  print(\"Features to drop:\", len(features_to_drop))\n",
        "  print(\"Reduced features:\", x_train_reduced.shape[1])\n",
        "  return x_train_reduced, x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3gOTUsp3kQJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "def reduce_df(x_tr, x_te, y_tr, reduction_choice, if_final):\n",
        "  if reduction_choice == \"PLS\":\n",
        "    pls = PLSRegression(n_components=3)\n",
        "    X_tr_pls = pls.fit_transform(x_tr, y_tr)[0]  #Extract transformed features\n",
        "    if x_te.empty:\n",
        "      X_te_pls = pd.DataFrame(columns=[\"PLS1\", \"PLS2\", \"PLS3\"])  #Create empty DataFrame with correct columns if x_te is empty\n",
        "    else:\n",
        "      X_te_pls = pls.transform(x_te)\n",
        "    X_tr_pls = pd.DataFrame(X_tr_pls, columns=[\"PLS1\", \"PLS2\", \"PLS3\"])\n",
        "    X_te_pls = pd.DataFrame(X_te_pls, columns=[\"PLS1\", \"PLS2\", \"PLS3\"])\n",
        "    #print(\"Explained variance in X:\", np.round(pls.x_scores_.var(axis=0) / x_tr.var(axis=0).sum(), 3))\n",
        "    #print(\"Explained variance in Y:\", np.round(pls.y_scores_.var(axis=0) / y_tr.var(), 3))\n",
        "    return X_tr_pls, X_te_pls, pls\n",
        "\n",
        "  if reduction_choice == \"PCA\":\n",
        "    pca=PCA(n_components = 3, random_state=28) #n_components = None, 420\n",
        "    X_tr_PCA = pca.fit_transform(x_tr)\n",
        "    if if_final == \"no\":\n",
        "      X_te_PCA = pca.transform(x_te)\n",
        "    else:\n",
        "      X_te_PCA = x_te\n",
        "    #print(\"Principal axes:\\n\", pca.components_.tolist())\n",
        "    #print(\"Explained variance:\\n\", pca.explained_variance_.tolist())\n",
        "    print(\"Mean:\", pca.mean_)\n",
        "    return X_tr_PCA, X_te_PCA, pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6Wz_o_8rgX0"
      },
      "outputs": [],
      "source": [
        "def get_inputs(data_frame, y, tr_index, te_index, scaler_choice, thresh, if_final):\n",
        "#Feature Importance:\n",
        "  if scaler_choice == \"MMS\":\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler2 = MinMaxScaler()\n",
        "  else:\n",
        "    scaler = StandardScaler()\n",
        "    scaler2 = StandardScaler()\n",
        "\n",
        "  if if_final == 'yes':\n",
        "    data_scaled_train, data_scaled_test, y_train, y_test = data_frame, pd.DataFrame(), y, pd.DataFrame()\n",
        "  else:\n",
        "    data_scaled_train, data_scaled_test, y_train, y_test = data_frame.iloc[tr_index], data_frame.iloc[[te_index]], y.iloc[tr_index], y.iloc[te_index]\n",
        "\n",
        "  train_scaled = pd.DataFrame(scaler.fit_transform(data_scaled_train), columns = data_frame.columns)\n",
        "  SOS_train_scaled = train_scaled.mul(train_scaled[\"SOS\"], axis=0)\n",
        "  SOS_train_scaled = SOS_train_scaled.add_suffix('tim_SOS')\n",
        "  train_scaled = pd.concat([train_scaled, SOS_train_scaled], axis=1)\n",
        "  y_train = pd.Series(scaler2.fit_transform(y_train.values.reshape(-1, 1)).flatten())\n",
        "  train_scaled_correlated, correlations_df = correlation(train_scaled, thresh, y_train) #\n",
        "  train_scaled_correlated = pd.DataFrame(train_scaled_correlated)\n",
        "\n",
        "  # prev_train_scaled_correlated = train_scaled_correlated\n",
        "  # most_corr = correlations_df.columns[0]\n",
        "  # second_most_corr = correlations_df.columns[1]\n",
        "  # third_most_corr = correlations_df.columns[2]\n",
        "  # most_corr_train = prev_train_scaled_correlated.mul(train_scaled_correlated[most_corr], axis=0)\n",
        "  # most_corr_train = most_corr_train.add_suffix(\"*\")\n",
        "  # most_corr_train = most_corr_train.add_suffix(most_corr)\n",
        "  # train_scaled_correlated = pd.concat([train_scaled_correlated, most_corr_train], axis=1)\n",
        "\n",
        "  # second_most_corr_train = prev_train_scaled_correlated.mul(train_scaled_correlated[second_most_corr], axis=0)\n",
        "  # second_most_corr_train = second_most_corr_train.add_suffix(\"*\")\n",
        "  # second_most_corr_train = second_most_corr_train.add_suffix(second_most_corr)\n",
        "  # train_scaled_correlated = pd.concat([train_scaled_correlated, second_most_corr_train], axis=1)\n",
        "  train_scaled_correlated, correlations_df = correlation(train_scaled_correlated, thresh, y_train)\n",
        "\n",
        "  if if_final == 'no':\n",
        "    y_test = pd.Series(y_test)\n",
        "    y_test = y_test.values.reshape(-1, 1)\n",
        "    y_test = scaler2.transform(y_test).flatten()\n",
        "    test_scaled = pd.DataFrame(scaler.transform(data_scaled_test), columns=data_frame.columns)\n",
        "    SOS_test_scaled = test_scaled.mul(test_scaled[\"SOS\"], axis=0)\n",
        "    SOS_test_scaled = SOS_test_scaled.add_suffix('tim_SOS')\n",
        "    test_scaled = pd.concat([test_scaled, SOS_test_scaled], axis=1)\n",
        "\n",
        "    # prev_test_scaled = test_scaled\n",
        "    # most_corr_test = prev_test_scaled.mul(test_scaled[most_corr], axis=0)\n",
        "    # most_corr_test = most_corr_test.add_suffix(\"*\")\n",
        "    # most_corr_test = most_corr_test.add_suffix(most_corr)\n",
        "    # test_scaled = pd.concat([test_scaled, most_corr_test], axis=1)\n",
        "\n",
        "    # second_most_corr_test = prev_test_scaled.mul(test_scaled[second_most_corr], axis=0)\n",
        "    # second_most_corr_test = second_most_corr_test.add_suffix(\"*\")\n",
        "    # second_most_corr_test = second_most_corr_test.add_suffix(second_most_corr)\n",
        "    # test_scaled = pd.concat([test_scaled, second_most_corr_test], axis=1)\n",
        "    test_scaled_correlated = test_scaled.loc[:, train_scaled_correlated.columns] #Test data with only correlated inputs\n",
        "  else:\n",
        "    test_scaled_correlated = data_scaled_test\n",
        "\n",
        "  train_scaled_correlated, test_scaled_correlated = corr_matrix_reduce(train_scaled_correlated, test_scaled_correlated)\n",
        "  correlations_df2 = correlations_df.loc[:, train_scaled_correlated.columns]\n",
        "\n",
        "  return train_scaled_correlated, test_scaled_correlated, scaler, scaler2, y_train, y_test, correlations_df#,correlations_df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIknVAOovdYx"
      },
      "outputs": [],
      "source": [
        "def reduce_and_model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, reduction_choice, scaler_target, is_final_model, model_choice):\n",
        "  if reduction_choice == 'PLS':\n",
        "    X_tr_reduced, X_te_reduced, PLS_reducer = reduce_df(X_tr_reduced, X_te_reduced, Y_tr, \"PLS\", is_final_model)\n",
        "  elif reduction_choice == 'PCA':\n",
        "    X_tr_reduced, X_te_reduced, PCA_reducer = reduce_df(X_tr_reduced, X_te_reduced, Y_tr, \"PCA\", is_final_model)\n",
        "\n",
        "  if model_choice == 'Ridge':\n",
        "    model = RLE_Model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, \"Ridge\", X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'Lasso':\n",
        "    model = RLE_Model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, \"Lasso\", X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'Elastic':\n",
        "    model = RLE_Model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, \"Elastic\", X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'RF':\n",
        "    model = RF_model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, X_te_reduced, scaler_target)\n",
        "  elif model_choice == 'XG':\n",
        "    if not isinstance(X_tr_reduced, pd.DataFrame):\n",
        "        X_tr_reduced = pd.DataFrame(X_tr_reduced)\n",
        "    if not isinstance(X_te_reduced, pd.DataFrame):\n",
        "        X_te_reduced = pd.DataFrame(X_te_reduced)\n",
        "    X_tr_reduced.columns = X_tr_reduced.columns.astype(str).str.replace(r'[\\[\\]<]', 'under', regex=True)\n",
        "    X_te_reduced.columns = X_te_reduced.columns.astype(str).str.replace(r'[\\[\\]<]', 'under', regex=True)\n",
        "    model = XG_model(X_tr_reduced, X_te_reduced, Y_tr, Y_te, X_te_reduced, scaler_target)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sZZxhEdi9Z4w"
      },
      "outputs": [],
      "source": [
        "from joblib import Parallel, delayed\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "# Define the function that processes each fold of LOO-CV and can make final model\n",
        "def process_fold(train_index, test_index, X, y, reduce_choice, corr_thresh, scaling_choice, modeling_choice):\n",
        "  if scaling_choice == \"MMS\":\n",
        "    X_train, X_test, scalerPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"MMS\", corr_thresh, 'no')\n",
        "  else:\n",
        "    X_train, X_test, scalerPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"\", corr_thresh, 'no')\n",
        "\n",
        "  common_columns = list(set(X_train.columns).intersection(correlations_df.columns))\n",
        "  if not common_columns:\n",
        "      # Handle the case where there are no correlated columns\n",
        "      return None\n",
        "  X_train_with_corrs = pd.concat([X_train[common_columns], correlations_df[common_columns]])\n",
        "  X_train_with_corrs = X_train_with_corrs.transpose()\n",
        "  X_train_with_corrs = X_train_with_corrs.sort_values(by='corrs', ascending = False, key=abs)\n",
        "  X_train_with_corrs = X_train_with_corrs.head(30)\n",
        "  X_train_reduced = X_train_with_corrs.drop('corrs', axis = 1) #drop corrs column\n",
        "  X_train_reduced = X_train_reduced.transpose()\n",
        "  X_train_reduced.reset_index(drop = True, inplace = True)\n",
        "  X_test_reduced = pd.DataFrame(X_test, columns=X_train_reduced.columns)\n",
        "\n",
        "  model = reduce_and_model(X_train_reduced, X_test_reduced, Y_train, Y_test, reduce_choice, scalerY, 'no', modeling_choice)\n",
        "\n",
        "  return model #Return the model for each fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWimHdH6khJZ"
      },
      "source": [
        "# **Test 1 Fold**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAYkKsONkguW"
      },
      "outputs": [],
      "source": [
        "X = inputs\n",
        "y = playoff_stats['GA/GP_Playoffs'] #GA/GP_Playoffs\n",
        "len_df = len(X)\n",
        "train_index = list(range(len_df-1))\n",
        "test_index = list(range((len_df-1), len_df))\n",
        "X_train, X_test, scalerPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"MMS\", 0.37, 'no')\n",
        "print(len(X_test.columns))\n",
        "common_columns = list(set(X_train.columns).intersection(correlations_df.columns))\n",
        "X_train_with_corrs = pd.concat([X_train[common_columns], correlations_df[common_columns]])\n",
        "X_train_with_corrs = X_train_with_corrs.transpose()\n",
        "X_train_with_corrs = X_train_with_corrs.sort_values(by='corrs', ascending = False, key=abs)\n",
        "X_train_with_corrs = X_train_with_corrs.head(30)\n",
        "X_train_reduced = X_train_with_corrs.drop('corrs', axis = 1) #drop corrs column\n",
        "X_train_reduced = X_train_reduced.transpose()\n",
        "X_train_reduced.reset_index(drop = True, inplace = True)\n",
        "X_test_reduced = pd.DataFrame(X_test, columns=X_train_reduced.columns)\n",
        "print(len(X_test_reduced.columns))\n",
        "\n",
        "#model = reduce_and_model(X_train_reduced, X_test_reduced, Y_train, Y_test, 'none', scalerY, 'no', 'Lasso')\n",
        "#print(model[4]) #test error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gimKSs5rrWWW"
      },
      "source": [
        "# **Run LOO-CV**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOgIPBpHZHFg"
      },
      "source": [
        "*   Each scaler for corr, PLS; PCA\n",
        "*   adjust tuning, only take linear corr columns, mult by top 3 most corr; run longer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baeBkT2AohI6",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "X = inputs\n",
        "y = playoff_stats['GA/GP_Playoffs']\n",
        "loo = LeaveOneOut()\n",
        "results = Parallel(n_jobs=-1)(delayed(process_fold)(train_idx, test_idx, X, y, 'PLS', .37, '', 'XG') #\n",
        "                              for train_idx, test_idx in loo.split(X))\n",
        "train_NRMSE_scores = 0\n",
        "train_NMAE_scores = 0\n",
        "test_RMSE_num_error = 0\n",
        "train_error = 0\n",
        "test_error = 0\n",
        "len_df = len(X)\n",
        "y_test_preds = []\n",
        "range_target = y.max() - y.min()\n",
        "\n",
        "for i in range(len_df):\n",
        "  train_NRMSE_scores = results[i][0] + train_NRMSE_scores\n",
        "  train_NMAE_scores = results[i][1] + train_NMAE_scores\n",
        "  train_error = results[i][2] + train_error #inv transformed\n",
        "  test_error = results[i][3] + test_error #inv transformed\n",
        "  test_RMSE_num_error = results[i][3]**2 + test_RMSE_num_error\n",
        "  y_test_preds.append(results[i][4]) #inv transformed\n",
        "\n",
        "test_MAE = test_error/len_df\n",
        "test_RMSE = math.sqrt(test_RMSE_num_error[0]/len_df)\n",
        "test_NRMSE = test_RMSE/range_target\n",
        "test_NMAE = test_MAE/range_target\n",
        "range_preds = max(y_test_preds) - min(y_test_preds)\n",
        "\n",
        "print(f'AVG training Normalized RMSE: {(train_NRMSE_scores/len_df):.2f}')\n",
        "print(f'AVG training Normalized MAE: {(train_NMAE_scores/len_df):.2f}')\n",
        "print(f'AVG of avg inv transformed train error from folds: {(np.mean(train_error)/len_df):.1f}')\n",
        "print(f'Test Normalized RMSE: {test_NRMSE:.2f}')\n",
        "print(f'Test Normalized MAE: {test_NMAE.item():.2f}')\n",
        "print(f'AVG inv transformed test error: {(test_error.item()/len_df):.2f}')\n",
        "print(f'Range of predictions (inv transformed): {(range_preds.item()):.1f}') #make sure not predicting same value for all preds\n",
        "print('Test Predictions (inv transformed):')\n",
        "for value in y_test_preds:\n",
        "    print(f'{value.item():.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fmkgLwhdFPi2"
      },
      "outputs": [],
      "source": [
        "print(f\"{range_target:.1f}\")\n",
        "y_test_preds_flat = np.array([pred.item() for pred in y_test_preds])  # Flatten the list of arrays\n",
        "result = y_test_preds_flat - y.values  # Convert y to a NumPy array for subtraction\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSUQdmSRnKO8"
      },
      "source": [
        "# **Final Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYtJ5AylwvFU"
      },
      "outputs": [],
      "source": [
        "X = inputs\n",
        "y = playoff_stats['GF/GP_Playoffs']\n",
        "len_df = len(X)\n",
        "train_index = list(range(len_df))\n",
        "test_index = list(range(1))\n",
        "\n",
        "X_train, X_test, scalerPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"MMS\", 0.355, 'yes')\n",
        "\n",
        "common_columns = list(set(X_train.columns).intersection(correlations_df.columns))\n",
        "X_train_with_corrs = pd.concat([X_train[common_columns], correlations_df[common_columns]])\n",
        "X_train_with_corrs = X_train_with_corrs.transpose()\n",
        "X_train_with_corrs = X_train_with_corrs.sort_values(by='corrs', ascending = False, key=abs)\n",
        "X_train_with_corrs = X_train_with_corrs.head(30)\n",
        "X_train_reduced = X_train_with_corrs.drop('corrs', axis = 1) #drop corrs column\n",
        "X_train_reduced = X_train_reduced.transpose()\n",
        "X_train_reduced.reset_index(drop = True, inplace = True)\n",
        "\n",
        "model = reduce_and_model(X_train_reduced, X_test, Y_train, Y_test, '', scalerY, 'yes', 'Elastic')\n",
        "\n",
        "range_preds =  model[5].max() - model[5].min() #inv transformed\n",
        "print(f'Normalized RMSE: {(model[0]):.3f}')\n",
        "print(f'Normalized MAE: {(model[1]):.3f}')\n",
        "print(f'avg inv transformed accuracy: {(np.mean(model[2])):.1f}')\n",
        "print(f'Range of predictions (inv transformed): {(range_preds):.1f}') #make sure not predicting same value for all preds\n",
        "print('inv transformed predictions:')\n",
        "for value in model[5]:\n",
        "    print(f'{value:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "range_preds =  model[5].max() - model[5].min() #inv transformed\n",
        "print(f'Normalized RMSE: {(model[0]):.3f}')\n",
        "print(f'Normalized MAE: {(model[1]):.3f}')\n",
        "print(f'avg inv transformed accuracy: {(np.mean(model[2])):.1f}')\n",
        "print(f'Range of predictions (inv transformed): {(range_preds):.1f}') #make sure not predicting same value for all preds\n",
        "print('inv transformed predictions:')\n",
        "for value in model[5]:\n",
        "    print(f'{value:.2f}')"
      ],
      "metadata": {
        "id": "xnGCWzy7wGaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_reduced.to_csv('GF_GP Inputs.csv')"
      ],
      "metadata": {
        "id": "eLo4B45qzass"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81Jm8_OhFIsD"
      },
      "outputs": [],
      "source": [
        "# import scipy.stats\n",
        "# X = inputs\n",
        "# y = playoff_stats['Pts/GM_Playoffs']#Pts/GM_Playoffs, oPts/GM_Playoffs #\n",
        "# col = ''\n",
        "# print(X.loc[:, col].corr(y))\n",
        "# print(scipy.stats.spearmanr(X.loc[:,col], y)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEgEYO_sTh3t"
      },
      "source": [
        "# **GF/GP**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghueImbtRM2r"
      },
      "source": [
        "**Elastic** - .355, 30, PCA MMS\n",
        "*  AVG training Normalized RMSE: 0.17\n",
        "*  AVG training Normalized MAE: 0.14\n",
        "*  AVG of avg inv transformed train error from folds: 0.3\n",
        "*  Test Normalized RMSE: 0.22\n",
        "*  Test Normalized MAE: 0.18\n",
        "*  AVG inv transformed test error: 0.3\n",
        "*  Range of predictions (inv transformed): 0.8\n",
        "\n",
        "**Elastic Final** - .355, 30, PCA MMS\n",
        "*  Normalized RMSE: 0.173\n",
        "*  Normalized MAE: 0.143\n",
        "*  avg inv transformed accuracy: 0.3\n",
        "*  Range of predictions (inv transformed): 0.7\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = inputs\n",
        "y = playoff_stats['GF/GP_Playoffs']#\n",
        "len_df = len(X)\n",
        "train_index = list(range(len_df))\n",
        "test_index = list(range(1))\n",
        "\n",
        "X_train, X_test, scaleroPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"MMS\", 0.355, 'yes')\n",
        "common_columns = list(set(X_train.columns).intersection(correlations_df.columns))\n",
        "X_train_with_corrs = pd.concat([X_train[common_columns], correlations_df[common_columns]])\n",
        "X_train_with_corrs = X_train_with_corrs.transpose()\n",
        "X_train_with_corrs = X_train_with_corrs.sort_values(by='corrs', ascending = False, key=abs)\n",
        "X_train_with_corrs = X_train_with_corrs.head(30)\n",
        "X_train_reduced = X_train_with_corrs.drop('corrs', axis = 1) #drop corrs column\n",
        "X_train_reduced = X_train_reduced.transpose()\n",
        "X_train_reduced.reset_index(drop = True, inplace = True)"
      ],
      "metadata": {
        "id": "yXX2knelZxkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GF_GP_predict = pd.read_csv('GF_GP Inputs.csv')\n",
        "GF_GP_predict = GF_GP_predict[0:16]\n",
        "GF_GP_scaler = MinMaxScaler()\n",
        "target_scaler = MinMaxScaler()\n",
        "trained_features_to_scale = X[GF_GP_predict.columns]\n",
        "trained_features_scaled = pd.DataFrame(GF_GP_scaler.fit_transform(trained_features_to_scale), columns = trained_features_to_scale.columns)\n",
        "\n",
        "GF_GP_predict_scaled = pd.DataFrame(GF_GP_scaler.transform(GF_GP_predict), columns = GF_GP_predict.columns)\n",
        "GF_GP_playoffs_scaled = pd.Series(target_scaler.fit_transform(y.values.reshape(-1, 1)).flatten())"
      ],
      "metadata": {
        "id": "jvnKCF-XZy06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GF_GP_predict_scaled['GF in P1tim_SOS'] = GF_GP_predict_scaled['GF in P1'] * GF_GP_predict_scaled['SOS']\n",
        "GF_GP_predict_scaled['PPA)/FO)tim_SOS'] = GF_GP_predict_scaled['PPA)/FO)'] * GF_GP_predict_scaled['SOS']\n",
        "GF_GP_predict_scaled['SO_Lg_AVG)/Shots)tim_SOS'] = GF_GP_predict_scaled['SO_Lg_AVG)/Shots)'] * GF_GP_predict_scaled['SOS']\n",
        "GF_GP_predict_scaled['PPA_Lg_AVG)/GM)tim_SOS'] = GF_GP_predict_scaled['PPA_Lg_AVG)/GM)'] * GF_GP_predict_scaled['SOS']\n",
        "GF_GP_predict_scaled['Net PK%tim_SOS'] = GF_GP_predict_scaled['Net PK%'] * GF_GP_predict_scaled['SOS']\n",
        "GF_GP_predict_scaled['PP TOI)/PIM)tim_SOS'] = GF_GP_predict_scaled['PP TOI)/PIM)'] * GF_GP_predict_scaled['SOS']\n",
        "\n",
        "GA_GP_predict_scaled = GF_GP_predict_scaled.drop('SOS', axis = 1)"
      ],
      "metadata": {
        "id": "VsTnYn6KZ0Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GF_GP_model = joblib.load('GF_GP_Elastic.pkl')\n",
        "\n",
        "pca=PCA(n_components = 3, random_state=28) #n_components = None, 420\n",
        "X_train_PCA = pca.fit_transform(X_train_reduced)\n",
        "GF_GP_predict_scaled = GF_GP_predict_scaled[X_train_reduced.columns]\n",
        "GF_GP_predict_scaled_PCA = pca.transform(GF_GP_predict_scaled)\n",
        "\n",
        "GF_GP_predictions = GF_GP_model.predict(GF_GP_predict_scaled_PCA)\n",
        "GF_GP_predictions = target_scaler.inverse_transform(GF_GP_predictions.reshape(-1, 1))\n",
        "GF_GP_predictions = pd.Series(GF_GP_predictions.flatten())\n",
        "GF_GP_predictions.to_csv('GF_GP_preds.csv')\n",
        "GF_GP_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "7iQBQ2HYZ1Tm",
        "outputId": "3bd3a42b-7e67-4dea-f743-f0bbac329384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     3.220374\n",
              "1     3.331965\n",
              "2     3.314416\n",
              "3     3.341655\n",
              "4     3.217394\n",
              "5     3.257215\n",
              "6     3.301197\n",
              "7     3.215157\n",
              "8     3.171326\n",
              "9     3.253342\n",
              "10    3.348755\n",
              "11    3.349830\n",
              "12    3.319649\n",
              "13    3.406351\n",
              "14    3.379635\n",
              "15    3.334025\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.220374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.331965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.314416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.341655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.217394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3.257215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3.301197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3.215157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3.171326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3.253342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3.348755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3.349830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3.319649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3.406351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3.379635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3.334025</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GA/GP**"
      ],
      "metadata": {
        "id": "SbsvQiTxTVlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lasso** - .37, 30, PCA, MMS\n",
        "*  AVG training Normalized RMSE: 0.14\n",
        "*  AVG training Normalized MAE: 0.11\n",
        "*  AVG of avg inv transformed train error from folds: 0.2\n",
        "*  Test Normalized RMSE: 0.19\n",
        "*  Test Normalized MAE: 0.15\n",
        "*  AVG inv transformed test error: 0.3\n",
        "*  Range of predictions (inv transformed): 0.7\n",
        "\n",
        "**Lasso Final**\n",
        "*  Best trial:\n",
        "  Params: {'alpha': 0.010002827238715567, 'max_iter': 7600, 'tol': 0.0007167933367591644}\n",
        "*  Normalized RMSE: 0.138\n",
        "*  Normalized MAE: 0.110\n",
        "*  avg inv transformed accuracy: 0.2\n",
        "*  Range of predictions (inv transformed): 0.9"
      ],
      "metadata": {
        "id": "FGF_pXwZ3fgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = inputs\n",
        "y = playoff_stats['GA/GP_Playoffs']#\n",
        "len_df = len(X)\n",
        "train_index = list(range(len_df))\n",
        "test_index = list(range(1))\n",
        "\n",
        "X_train, X_test, scaleroPPG, scalerY, Y_train, Y_test, correlations_df = get_inputs(X, y, train_index, test_index[0], \"MMS\", 0.37, 'yes')\n",
        "common_columns = list(set(X_train.columns).intersection(correlations_df.columns))\n",
        "X_train_with_corrs = pd.concat([X_train[common_columns], correlations_df[common_columns]])\n",
        "X_train_with_corrs = X_train_with_corrs.transpose()\n",
        "X_train_with_corrs = X_train_with_corrs.sort_values(by='corrs', ascending = False, key=abs)\n",
        "X_train_with_corrs = X_train_with_corrs.head(30)\n",
        "X_train_reduced = X_train_with_corrs.drop('corrs', axis = 1) #drop corrs column\n",
        "X_train_reduced = X_train_reduced.transpose()\n",
        "X_train_reduced.reset_index(drop = True, inplace = True)"
      ],
      "metadata": {
        "id": "YutMh5GaZsUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GA_GP_predict = pd.read_csv('GA_GP Inputs.csv')\n",
        "GA_GP_scaler = MinMaxScaler()\n",
        "target_scaler = MinMaxScaler()\n",
        "trained_features_to_scale = X[GA_GP_predict.columns]\n",
        "trained_features_scaled = pd.DataFrame(GA_GP_scaler.fit_transform(trained_features_to_scale), columns = trained_features_to_scale.columns)\n",
        "\n",
        "GA_GP_predict_scaled = pd.DataFrame(GA_GP_scaler.transform(GA_GP_predict), columns = GA_GP_predict.columns)\n",
        "GA_GP_playoffs_scaled = pd.Series(target_scaler.fit_transform(y.values.reshape(-1, 1)).flatten())"
      ],
      "metadata": {
        "id": "qUs6DQKwZtZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GA_GP_predict_scaled['PP%_Lg_AVGtim_SOS'] = GA_GP_predict_scaled['PP%_Lg_AVG'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['GF in P1tim_SOS'] = GA_GP_predict_scaled['GF in P1'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['EV GFtim_SOS'] = GA_GP_predict_scaled['EV GF'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['GF in P1)/PP TOI)tim_SOS'] = GA_GP_predict_scaled['GF in P1)/PP TOI)'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['EV GF)/Shots)tim_SOS'] = GA_GP_predict_scaled['EV GF)/Shots)'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['GF_Lg_AVG)/GM)tim_SOS'] = GA_GP_predict_scaled['GF_Lg_AVG)/GM)'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['S_Lg_AVG)/GM)tim_SOS'] = GA_GP_predict_scaled['S_Lg_AVG)/GM)'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['GA)/GM)tim_SOS'] = GA_GP_predict_scaled['GA)/GM)'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['GA in P2)/GM)tim_SOS'] = GA_GP_predict_scaled['GA in P2)/GM)'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['PP%_Lg_AVG)/FO)tim_SOS'] = GA_GP_predict_scaled['PP%_Lg_AVG)/FO)'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['PK%_Lg_AVGtim_SOS'] = GA_GP_predict_scaled['PK%_Lg_AVG'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['BkS/60)/FO)tim_SOS'] = GA_GP_predict_scaled['BkS/60)/FO)'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['GA in P3)/PP TOI)tim_SOS'] = GA_GP_predict_scaled['GA in P3)/PP TOI)'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['Msct_Pen)/PP TOI)tim_SOS'] = GA_GP_predict_scaled['Msct_Pen)/PP TOI)'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['SO_Lg_AVGtim_SOS'] = GA_GP_predict_scaled['SO_Lg_AVG'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['PPA)/PP TOI)tim_SOS'] = GA_GP_predict_scaled['PPA)/PP TOI)'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['Bench_Pen)/PP TOI)tim_SOS'] = GA_GP_predict_scaled['Bench_Pen)/PP TOI)'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['EV_BP/GM)/PP TOI)tim_SOS'] = GA_GP_predict_scaled['EV_BP/GM)/PP TOI)'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['GA in P2)/Shots)tim_SOS'] = GA_GP_predict_scaled['GA in P2)/Shots)'] * GA_GP_predict_scaled['SOS']\n",
        "GA_GP_predict_scaled['SH FOW%)/Shots)tim_SOS'] = GA_GP_predict_scaled['SH FOW%)/Shots)'] * GA_GP_predict_scaled['SOS']\n",
        "\n",
        "GA_GP_predict_scaled = GA_GP_predict_scaled.drop('SOS', axis = 1)"
      ],
      "metadata": {
        "id": "AselpffDZufA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GA_GP_model = joblib.load('GA_GP_Lasso.pkl')\n",
        "\n",
        "pca=PCA(n_components = 3, random_state=28) #n_components = None, 420\n",
        "X_train_PCA = pca.fit_transform(X_train_reduced)\n",
        "GA_GP_predict_scaled = GA_GP_predict_scaled[X_train_reduced.columns]\n",
        "GA_GP_predict_scaled_PCA = pca.transform(GA_GP_predict_scaled)\n",
        "\n",
        "GA_GP_predictions = GA_GP_model.predict(GA_GP_predict_scaled_PCA)\n",
        "GA_GP_predictions = target_scaler.inverse_transform(GA_GP_predictions.reshape(-1, 1))\n",
        "GA_GP_predictions = pd.Series(GA_GP_predictions.flatten())\n",
        "GA_GP_predictions.to_csv('GA_GP_preds.csv')\n",
        "GA_GP_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "f0k6TeDnZvuL",
        "outputId": "faf8e4b0-0692-41bd-8c65-ee93d158c031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     3.096871\n",
              "1     3.685642\n",
              "2     3.157840\n",
              "3     3.439815\n",
              "4     3.198052\n",
              "5     3.086986\n",
              "6     3.491820\n",
              "7     3.655494\n",
              "8     2.981638\n",
              "9     3.065964\n",
              "10    3.816695\n",
              "11    3.585518\n",
              "12    3.844526\n",
              "13    3.519401\n",
              "14    3.140305\n",
              "15    3.360474\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.096871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.685642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.157840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.439815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.198052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3.086986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3.491820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3.655494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2.981638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3.065964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>3.816695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3.585518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3.844526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3.519401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>3.140305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3.360474</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "l9zr28DwjQ6D",
        "d4dHDJtOsKU_",
        "k4QbyjhZi-0Z",
        "dWimHdH6khJZ",
        "gimKSs5rrWWW",
        "KSUQdmSRnKO8",
        "OEgEYO_sTh3t"
      ],
      "provenance": [],
      "mount_file_id": "1Vm8WmlMWl5qPkDh4nqbzn64vOM6ndqxq",
      "authorship_tag": "ABX9TyP4fl3O9M8jOFbG3/L6L60Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}